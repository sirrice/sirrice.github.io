<!doctype html>
<html>
<head>
<meta charset="utf-8">
<style>
/*--------------------- Layout and Typography ----------------------------*/
body {
  font-family: 'Palatino Linotype', 'Book Antiqua', Palatino, FreeSerif, serif;
  font-size: 16px;
  line-height: 24px;
  color: #252519;
  margin: 0; padding: 10px;
}
a {
  color: #261a3b;
}
  a:visited {
    color: #261a3b;
  }
p {
  margin: 0 0 15px 0;
}
h1, h2, h3, h4, h5, h6 {
  margin: 40px 0 15px 0;
}
h2, h3, h4, h5, h6 {
    margin-top: 0;
  }
#container {
  position: relative;
}
#background {
  position: fixed;
  /*top: 0; left: 580px; right: 0; bottom: 0;*/
  background: #f5f5ff;
  border-left: 1px solid #e5e5ee;
  z-index: -1;
}
#jump_to, #jump_page {
  background: white;
  -webkit-box-shadow: 0 0 25px #777; -moz-box-shadow: 0 0 25px #777;
  -webkit-border-bottom-left-radius: 5px; -moz-border-radius-bottomleft: 5px;
  font: 10px Arial;
  text-transform: uppercase;
  cursor: pointer;
  text-align: right;
}
#jump_to, #jump_wrapper {
  position: fixed;
  right: 0; top: 0;
  padding: 5px 10px;
}
  #jump_wrapper {
    padding: 0;
    display: none;
  }
    #jump_to:hover #jump_wrapper {
      display: block;
    }
    #jump_page {
      padding: 5px 0 3px;
      margin: 0 0 25px 25px;
    }
      #jump_page .source {
        display: block;
        padding: 5px 10px;
        text-decoration: none;
        border-top: 1px solid #eee;
      }
        #jump_page .source:hover {
          background: #f5f5ff;
        }
        #jump_page .source:first-child {
        }
table td {
  border: 0;
  outline: 0;
}
  div.docs, td.docs, th.docs {
    padding: 10px 25px 1px 50px;
    vertical-align: top;
    text-align: left;
    width: 90%;
    border-bottom: 1px solid #e5e5ee;
    border-top: 1px solid #e5e5ee;
  }
    .docs pre {
      margin: 15px 0 15px;
      padding-left: 15px;
    }
    .docs p tt, .docs p code {
      background: #f8f8ff;
      border: 1px solid #dedede;
      font-size: 12px;
      padding: 0 0.2em;
    }
    /*.octowrap {
      position: relative;
    }*/
      .octothorpe {
        font: 12px Arial;
        text-decoration: none;
        color: #454545;
        position: absolute;
        top: 3px; left: -20px;
        padding: 1px 2px;
        opacity: 0;
        -webkit-transition: opacity 0.2s linear;
      }
        div.docs:hover, td.docs:hover .octothorpe {
          opacity: 1;
        }
  div.code, td.code, th.code {
    padding: 14px 15px 16px 100px;
    width: 100%;
    vertical-align: top;
    background: #f5f5ff;
  }
.code pre, .docs p code {
  font-size: 12px;
}
    pre, tt, code {
      line-height: 18px;
      font-family: Monaco, Consolas, "Lucida Console", monospace;
      margin: 0; padding: 0;
    }
pre {
  margin:1em 0;
  margin-left:-10px;
  margin-right: -10px;
  font-size:12px;
  background-color:#eee;
  border:1px solid #ddd;
  padding:10px;
  line-height:1.5em;
  color:#333;
  overflow:auto;
  -webkit-box-shadow:rgba(0,0,0,0.07) 0 1px 2px inset;
  -webkit-border-radius:3px;
  -moz-border-radius:3px;border-radius:3px;
}
pre code {
  padding:0px;
  font-size:12px;
  background-color:#eee;
  border:none;
}
code {
  font-size:12px;
  background-color:#f8f8ff;
  color:#333;
  padding:0 .2em;
  border:1px solid #dedede;
}



/*---------------------- Syntax Highlighting -----------------------------*/
td.linenos { background-color: #f0f0f0; padding-right: 10px; }
span.lineno { background-color: #f0f0f0; padding: 0 5px 0 5px; }
body .hll { background-color: #ffffcc }
body .c { color: #408080; font-style: italic }  /* Comment */
body .err { border: 1px solid #FF0000 }         /* Error */
body .k { color: #954121 }                      /* Keyword */
body .o { color: #666666 }                      /* Operator */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 }                     /* Comment.Preproc */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 }                     /* Generic.Deleted */
body .ge { font-style: italic }                 /* Generic.Emph */
body .gr { color: #FF0000 }                     /* Generic.Error */
body .gh { color: #000080; font-weight: bold }  /* Generic.Heading */
body .gi { color: #00A000 }                     /* Generic.Inserted */
body .go { color: #808080 }                     /* Generic.Output */
body .gp { color: #000080; font-weight: bold }  /* Generic.Prompt */
body .gs { font-weight: bold }                  /* Generic.Strong */
body .gu { color: #800080; font-weight: bold }  /* Generic.Subheading */
body .gt { color: #0040D0 }                     /* Generic.Traceback */
body .kc { color: #954121 }                     /* Keyword.Constant */
body .kd { color: #954121; font-weight: bold }  /* Keyword.Declaration */
body .kn { color: #954121; font-weight: bold }  /* Keyword.Namespace */
body .kp { color: #954121 }                     /* Keyword.Pseudo */
body .kr { color: #954121; font-weight: bold }  /* Keyword.Reserved */
body .kt { color: #B00040 }                     /* Keyword.Type */
body .m { color: #666666 }                      /* Literal.Number */
body .s { color: #219161 }                      /* Literal.String */
body .na { color: #7D9029 }                     /* Name.Attribute */
body .nb { color: #954121 }                     /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold }  /* Name.Class */
body .no { color: #880000 }                     /* Name.Constant */
body .nd { color: #AA22FF }                     /* Name.Decorator */
body .ni { color: #999999; font-weight: bold }  /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold }  /* Name.Exception */
body .nf { color: #0000FF }                     /* Name.Function */
body .nl { color: #A0A000 }                     /* Name.Label */
body .nn { color: #0000FF; font-weight: bold }  /* Name.Namespace */
body .nt { color: #954121; font-weight: bold }  /* Name.Tag */
body .nv { color: #19469D }                     /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold }  /* Operator.Word */
body .w { color: #bbbbbb }                      /* Text.Whitespace */
body .mf { color: #666666 }                     /* Literal.Number.Float */
body .mh { color: #666666 }                     /* Literal.Number.Hex */
body .mi { color: #666666 }                     /* Literal.Number.Integer */
body .mo { color: #666666 }                     /* Literal.Number.Oct */
body .sb { color: #219161 }                     /* Literal.String.Backtick */
body .sc { color: #219161 }                     /* Literal.String.Char */
body .sd { color: #219161; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #219161 }                     /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold }  /* Literal.String.Escape */
body .sh { color: #219161 }                     /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold }  /* Literal.String.Interpol */
body .sx { color: #954121 }                     /* Literal.String.Other */
body .sr { color: #BB6688 }                     /* Literal.String.Regex */
body .s1 { color: #219161 }                     /* Literal.String.Single */
body .ss { color: #19469D }                     /* Literal.String.Symbol */
body .bp { color: #954121 }                     /* Name.Builtin.Pseudo */
body .vc { color: #19469D }                     /* Name.Variable.Class */
body .vg { color: #19469D }                     /* Name.Variable.Global */
body .vi { color: #19469D }                     /* Name.Variable.Instance */
body .il { color: #666666 }                     /* Literal.Number.Integer.Long */

</style>
<title>Explanatory Provenance</title>
</head>
<body>
<h1>Explanatory Provenance</h1>

<p>Explanatory provenance is an extension to &ldquo;how&rdquo; provenance typified by <a href="">Trio</a> that orders provenance results by relevance.  In the database context, the provenance of a query result are the tuples from the base tables that were used to compute the result.</p>

<p>For example, consider a table containing readings (temperature, humidity) from 50 sensors grouped into 30 minute windows.  The following SQL aggregate query returns the average temperature in each window:</p>

<pre><code>    SELECT avg(temp), windowid 
    FROM readings 
    GROUP BY windowid 
    ORDER BY windowid
</code></pre>

<p>Suppose the user pointed at an output result</p>

<pre><code>    (100°, 3)  // 100 degrees, window 3
</code></pre>

<p>and wanted to know its provenance.  Conventional &ldquo;how&rdquo; provenance would return all of the 5000 sensor readings collected in the 30 minute window.  Unfortunately, this doesn&rsquo;t help the user identify the source of errors.  What&rsquo;s more helpful are descriptions of inputs that may be causing the error.  For example an explanation of <code>sensorid = 15</code> is more helpful.</p>

<p>We are interested in:</p>

<ol>
<li><strong>Efficient techniques</strong> for computing explanatory provenance</li>
<li>The APIs necessary to <strong>augment existing visualization interfaces</strong> (e.g., bar charts, scatter plots, etc) with support for explanator provenance</li>
</ol>


<h1>Current Problem Statement</h1>

<p>The overall problem is to construct simple predicates that explain user defined anomalies in the result of aggregate operators.   In particular, we are concerned with error resulting from <em>systematic addition</em> of bad data.</p>

<p><em>Systematic</em> in the sense that there exists a small constant number of conjuctive range predicates that describe the bad data.</p>

<p><em>Addition</em> in the sense that we only consider errors that can be fixed by <em>removing</em> subsets of the data; we do not consider error that can be fixed by <em>perturbing</em> data.</p>

<h2>Problem Setup</h2>

<p><strong>Given</strong></p>

<ol>
<li>A dataset <em>D</em> with <em>d</em> attributes (dimensions)</li>
<li>Aggregate function <em>A</em></li>
<li>Initial output <em>A(D)</em></li>
<li>The set of possible predicates, <em>P</em>, over D.</li>
</ol>


<p><strong>Let</strong> a predicate <em>p</em> &isin; <em>P</em> be defined as the conjunction of range clauses on the columns in <em>D</em>, with the constraint that each column is present in at most one clause.  Also, <em>p(D)</em> &sube; D is the set of tuples in <em>D</em> that satisfy the predicate <em>p</em>.  Since a predicate is essentially the describes a bounding box in the tuple space, we use the two interchangably.</p>

<p><strong>Let</strong> err(S &sube; D, D) be a user defined error function where</p>

<blockquote><p>err(S, D) &isin; [0,1] or NaN</p></blockquote>

<p>err(S, D) takes values as follows:</p>

<ol>
<li>err(S, D) = 1 when <em>A(D-S)</em> = <em>A(D)</em></li>
<li>err(S, D) = 0 when <em>A(D-S)</em> = a user desired value.</li>
<li>err(S, D) = NaN if S is invalid (e.g., S = D)</li>
</ol>


<p><strong>Let</strong> ε(p, D) = f(err(p(D), D), |p|, |p(D)|) be the composite error function where it is a combination of the user defined error, the size of <em>p(D)</em>, and the complexity of the predicate, <em>|p|</em> (i.e., the number of clauses).</p>

<p>For example, ε may be defined as follows, which places more emphasis on the predicate complexity:</p>

<blockquote><p>ε(p, D) = err(p(D), D) * |p(D)| * |p|<sup>2</sup></p></blockquote>

<p>Note that <em>err()</em> may take a non-negligable amount of time to execute.</p>

<h2>Goal</h2>

<p>The goal is to efficiently and accurately construct a <em>p* &isin; P</em> where:</p>

<blockquote><p>p* = argmin<sub>p &isin; P</sub> ε(p, D)</p></blockquote>

<h1>Performance Related Approaches</h1>

<h2>Exhaustive</h2>

<h4>Approach</h4>

<p>The first approach is to exhaustively enumerate and evaluate all possible predicates.  This approach can list all single attribute clauses, and enumerate all possible combinations of clauses.  The user can specify a maximum predicate complexity,f <em>N</em>, to limit the search space to all possible predicates <em>p</em> where <em>|p| &lt;= N</em>.</p>

<h4>Analysis</h4>

<blockquote><p>This approach is exponential.</p></blockquote>

<h2>Sampling Based</h2>

<h4>Assumptions</h4>

<ol>
<li>The error of each tuple in <em>D</em> is independent.  Note that this independence is defined by <em>err</em> (and subsequently, <em>A</em>),</li>
</ol>


<h4>Approach</h4>

<p>Given the above assumption, we can consider a variant of the problem statement that instead minimizes ε':</p>

<blockquote><p>ε'(p, D) = E[ ε({pt}, D) | pt &isin; p(D)]</p></blockquote>

<p>We can then estimate ε' using standard sampling based techniques.</p>

<p>We use a top down approach to iteratively partition the data space and identify partitions with high ε'.  The partitions with high ε' are then merged to construct the final predicate.</p>

<p>The pseudo-code of the top down algorithm is:</p>

<pre><code>    function top_down(partition, predicates, D)
        samples = take_sample(partition)
        errors = compute_ε'(samples, D)

        if good_enough(errors)
            predicates.add(bounding_box(partition))
            return

        best_d, split_point = pick_best_dimension(samples)
        left = {pt | pt ∈ samples ^ pt.best_d &lt;= split_point}
        right = {pt | pt ∈ samples ^ pt.best_d &lt; split_point}

        top_down(left, predicates, D)
        top_down(right, predicates, D)
</code></pre>

<p>The algorithm first estimates ε' using a sample of the partition.  If the variance of the estimate is within a threshold, then we can set the error of every tuple in the partition to the estimate.  Otherwise, the sample is used to pick the optimal partition to split on.</p>

<p>The goal is to, with high confidence, identify bounding boxes around partitions, <em>p</em>, where</p>

<ol>
<li><em>ε'(p, D)</em> is &le; 5% percentile</li>
<li>The expected mean squared error (MSE) less than a tight threshold

<blockquote><p>E[ ( ε({pt},D) - ε'(p, D) )<sup>2</sup> ] &lt; threshold where pt &isin; p</p></blockquote></li>
</ol>


<h4>How to compute the sample?</h4>

<p>Two ways to compute the sample:</p>

<h5>Search Based</h5>

<p>The first is search based &ndash; take just enough samples to be confident that we have not missed any bad tuples.  Because of the assumption that bad tuples will be clustered together, we simply need at least one sample from each group of bad tuples.</p>

<p>Thus the user specifies a percentage of the dataset, <em>t</em>, such that, the sample will, with high confidence, contain one or more bad tuples, assuming that there are <em>&ge; t*|D|</em>  bad tuples in <em>D</em>.</p>

<p><strong>Let</strong> <em>samp_size</em> be the sample size, and <strong>conf</strong> be the level of confidence.</p>

<p>Then <em>samp_size</em> is the minimum number such that the probability of picking <em>samp_size</em> non-bad tuples is lower than 1 - <em>conf</em>:</p>

<blockquote><p>( (1-t)*|D| choose samp_size ) / ( |D| choose samp_size ) &lt; (1 - conf)</p></blockquote>

<h5>Estimator Based</h5>

<p>The second alternative is to incrementally increase the sample size until the confidence interval of <em>variance(ε')</em> is below a user defined threshold.  This can be done by using the jackknife to estimate the confidence interval.  This is expected to require more samples than the search based approach.</p>

<p>Note that we are sampling to reduce the confidence interval of the variance &ndash; not to reduce the expected variance.  In particular, we are interested in whether or not a high or low variance is due to chance.</p>

<h4>When to terminate?</h4>

<p>Regardless of the sampling technique, the recursion terminates when the MSE/variance is below a threshold.  For example, the threshold could be 0.5%.</p>

<p>However, one key observation is that the threshold does not need to be uniform &ndash; since we are primarily interested in partitions with low ε' values the algorithm can terminate when it is confident that a partition doesn&rsquo;t contain low ε values.  This relaxed constraint allows the algorithm to increase the threshold when ε' is high.</p>

<h4>Picking the Dimension and Split Point</h4>

<p>Each call to <em>topdown</em> splits the current partition into left and right halves.  The current approach is to pick the best single dimension to split along the mean value.  The term &ldquo;best&rdquo; is defined as the split that maximizes the expected number of points that the algorithm can terminate on in the next iteration.</p>

<p>Specifically, for <em>split<sub>i</sub></em>, that generates the next partitions <em>left<sub>i</sub></em> and <em>right<sub>i</sub></em>,  we can compute the expected number of points that the search will terminate on:</p>

<blockquote><p>&sum;<sub>partition &isin; {left<sub>i</sub>, right<sub>i</sub>}</sub> P(MSE(partition) &lt; threshold) * |partition|</p></blockquote>

<p>The algorithm can then pick the <em>split<sub>i</sub></em> that maximizes this value.</p>

<p>Note: we also tried picking the <em>split<sub>i</sub></em> that minimizes the MSE(partition) of the next iteration, but that easily led to degenerate predicates.</p>

<h4>Analysis</h4>

<p>In the worst case, where bad tuples are smeared throughout the data space, this approach requires O(|D|) evaluations of <em>top_down</em>, and consequently ε.  However in the general case, it requires much fewer evaluations.</p>

<h2>Sorting Based</h2>

<p>The independence assumption used in the sampling based approach suggests an alternative approach for when computing ε' is very cheap.  We can compute ε' on every tuple in parallel, and sort the dataset by ε'.  The user can set a threshold to differentiate the good from bad tuples.</p>

<p>Once the tuples are labelled, standard decision tree learning or subgroup discovery algorithms can be used to construct predicates.</p>

<h2>Merging Based</h2>

<p>The assumption for the sampling based approach may not always hold for all aggregator functions.  The merging based approach still employs assumptions to avoid enumerating all possible predicates, however it is more costly than the sampling based approach.</p>

<h4>Assumption</h4>

<ol>
<li>ε is continuous with respect to adding and removing tuples.

<blockquote><p>ε(S &cup; A) > ε(S &cup; B) &hArr; ε(A) > ε(B)</p></blockquote>

<p>In other words, although tuples may not be independent, the union of two low ε tuples is still low.</p></li>
</ol>


<h4>Approach</h4>

<p>Bottom up approach uses the following merged-based algorithm:</p>

<pre><code>    D = dataset
    C = initial set of predicates
    new_clusters = {}
    loop:
        for c in C:
            c' = find_best_mergable_cluster(c, C)
            m = merge(c', c)
            if evaluate(ε(m, D))
                new_clusters add m

        if |new_clusters| = 0
            break
        C = C ∪ new_clusters
</code></pre>

<p>The algorithm starts with an initial set of predicates, and iteratively merges pairs of predicates until no pairs are merged.  A pair of predicates is merged by computing the minimum bounding rectangle of the two predicates and the subsequent ε.</p>

<p>Unlike the sampling based approach, it is not possible to memoize ε for individual tuples and reuse the calculations.  Thus ε must be executed for every new predicate.  In addition, the merging step can be very expensive, so it is important to consider situations where optimizations can be performed.</p>

<p>An additional difficulty is dealing with discrete attributes.  Since the distance between all discrete values are equal, it is not clear how to merge predicates containing different discrete values.  One possibility is to first partition <em>D</em> along the discrete dimensions, run the merging algorithm on each partition separately, and finally compare the predicates from across the partitions.</p>

<h4>Constructing the initial set of predicates</h4>

<p>There are three approaches to constructing the initial set of predicates:</p>

<h5>Naiive</h5>

<p>The naiive approach is to consider each individual tuple as an initial predicate.</p>

<h5>Nearest Neighbor</h5>

<p>A nearest neighbor is to sample <em>N</em> tuples from <em>D</em> and construct a predicate for each sample as the minimum bounding box containing the nearest neighbors within a radius <em>r</em> around the sample.  There are two constraints</p>

<ol>
<li><em>N</em> and <em>r</em> need to be set to have sufficient coverage.  E.g., &ge; 90% of the bad tuples are contained in some predicate.</li>
<li><em>r</em> is small enough that the bad tuples are concentrated in a small number of predicates.</li>
</ol>


<h5>L2 Voronoi Decomposition</h5>

<p>The Nearest Neighbor approach does not guarantee that every tuple is assigned a predicate.  An alternative is to perform a voronoi decomposition of <em>D</em> with <em>N</em> centroids, where the centroids are tuples sampled from <em>D</em>.  The downside is that this does not scale for larger numbers of dimensions.</p>

<h4>Analysis</h4>

<p>The cost of constructing an efficient index is O(n log n), while the cost of each nearest neighbor query is O(n<sup>1-1/d</sup>).  Recall that <em>d</em> is the number of attributes in <em>D</em> and C is the initial set of predicates.  Thus, the cost of creating the initial predicates is:</p>

<blockquote><p>O(n log n + |C| * n<sup>1-1/d</sup>)</p></blockquote>

<p>Constructing the efficient index can be performed offline, thus the cost is</p>

<blockquote><p>O(|C| * n<sup>1-1/d</sup>)</p></blockquote>

<p>The merging step can, in the worst case, require log<sub>2</sub>C iterations.  Each iteration builds a nearest neighbor index on <em>C</em> to use for <em>find_best_mergable_cluster</em>.   Thus the cost is:</p>

<blockquote><p>O(log<sub>2</sub>C * (|C| log |C| + |C| * |C|<sup>1-1/d</sup>))</p></blockquote>

<p>In addition, if the running time of ε is non-trivial, this approach will evaluate ε</p>

<blockquote><p>O(|C| + |C| log<sub>2</sub>C) times</p></blockquote>

<h4>Optimization Opportunities</h4>

<p>Clearly, a naiive merge based approach has undesirable performance characteristics, however there are a number of possible optimizations to dramatically reduce the costs.</p>

<ol>
<li>When ε is homomorphic, or more generally, when there exists <em>g()</em>, such that

<blockquote><p><em>g(ε(S1, D), ε(S2, D)) = ε(S1 &cup; S2, D)</em></p></blockquote>

<p>This allows the algorithm to compute ε once for each initial cluster, and apply <em>g</em> during the merging phase.  When ε is expensive to evaluate compared to <em>g</em>, this is a valuable optimization.</p></li>
<li>Reduce the number of initial clusters.</li>
<li>Efficiently find optimal clusters to merge (optimize <em>find_best_mergable_cluster</em>)</li>
<li>The algorithm only considers merging high ε predicates for merging, which by definition are a very small proportion of the dataset.</li>
<li>Use Locality Sensitive Hashing to efficiently compute r-NN on high dimensional data.</li>
<li>Barzan suggests application of the <a href="http://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf">Johnson–Lindenstrauss lemma</a>.

<blockquote><p>   The Johnson-Lindenstrauss Lemma describes projections of high dimensional datasets into lower dimension spaces such that pair-wise distances are preserved.</p></blockquote></li>
</ol>


<h2>Hybrid</h2>

<p>It&rsquo;s generally a good idea to consider a hybrid of different approaches.  One possibility is to use the top down approach over just the discrete attributes to identify the discrete partitions containing bad tuples, and then using the bottom-up approach to identify subsets of data within the partitions.</p>

<p>The main difficulty in this approach is reconciling the use of ε' in the top down approach and ε in the bottom up approach.  It is currently not clear how the following are related:</p>

<blockquote><p><em>E[ ε({pt}, D) | pt ∈ p(D)]</em> <strong>vs</strong> <em>ε(p(D), D)</em></p></blockquote>

<p>The goal is to show for certain sets of error functions, that their relative orderings are equivalent.</p>

<h1>Improving Quality (ideas)</h1>

<p>Techniques to improve the quality of the descriptions.</p>

<ol>
<li>Using a &ldquo;good&rdquo; dataset to perform significance testing</li>
<li>Returning sets of distinct predicatess</li>
</ol>


<h3>Significance Testing</h3>

<p>This is a variation of the problem where the user additionally provides a dataset, <em>G</em>, that contains &ldquo;normal&rdquo; points.  The intuition is that a predicate that affects both D and G is not significant, whereas a predicate that affects G but not D is descriptive of bad tuples unique to D.</p>

<ol>
<li>ε(p, D) is low</li>
<li>ε(p, G) &asymp; ε(FALSE, G)</li>
</ol>


<h3>Distinct Results</h3>

<p>An end user application would often like high <em>variety</em> in the predicate set.  That is, rather than 5 predicates describing the same set of tuples, the user would like 5 predicates describing different sets of tuples.</p>

<p>We need a metric to capture this desire.</p>

<h1>Alternative Approaches</h1>

<p>There are several alternative approaches that are often used for performing anomaly detection.</p>

<h2>SVM-based</h2>

<p>Support Vector Machines are a go-to technique for constructing a binary classifier using features in a dataset.  There are a number of reasons why we did not pursue this approach</p>

<ol>
<li>SVMs are a predictive tool instead of a explanatory tool.  The result is a classifier to predict if new tuples are bad, but the list of feature weights are difficult to understand.</li>
<li>SVMs rely on a labeled training/cross-validation dataset.  One of the problem constraints is that</li>
</ol>


<h1>Additional list of optimizations</h1>

<h2>Random Problems</h2>

<p>Leverage database particulars for this problem.  Some possibilities are:</p>

<ol>
<li>Prefetching</li>
<li>Paralellization</li>
<li>Materialization</li>
<li>Schema information</li>
<li>Query usage information</li>
<li>Functional dependency information

<ul>
<li>prune attributes along a functional dependency chain</li>
</ul>
</li>
<li>Correlation.  Prune attributes that are highly correlated.</li>
</ol>


<h1>Use Cases and Datasets</h1>

<ul>
<li>Data placement?</li>
<li>Data partitioning</li>
</ul>


<p>Aggregate operators</p>

<ol>
<li>avg/mean/stddev</li>
<li>linear regression</li>
<li>decision trees</li>
<li>from <a href="http://www.splunk.com">Splunk</a>

<ul>
<li>count</li>
<li>mode</li>
<li>count distinct</li>
<li>max</li>
<li>min</li>
</ul>
</li>
</ol>


<p>Datasets</p>

<ol>
<li>MGH health dataset</li>
<li>Intel sensor dataset</li>
<li>FEC election financing dataset</li>
<li>Boston crime reports dataset</li>
<li>Boston parking tickets violations dataset</li>
<li>UCI ML datasets</li>
<li>Locu&rsquo;s human computation workflow data

<ul>
<li>their ML procedules compute features and perform an initial image-to-structured-data pass on restaurant menu pages.  Workers then look through the results and clean the data.</li>
<li>Hierarchical worker structure of workers, leaders and managers</li>
<li>Given a bad worker&rsquo;s results, what are the reasons?  Is it due to the worker or the data?</li>
</ul>
</li>
</ol>


<h1>Related Work</h1>

<p>Meliou et al: Integrating constraint solvers to databases.</p>

<p>Amol Deshpande et al.  Sensitivity analysis for probabilistic provenance</p>

<p>Approximate Top-K.</p>
</body>
</html>